{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fc68a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,LeakyReLU,Reshape,Flatten,Input\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e791bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28,28,1)  # mnist have black and white images of size 28,28  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89efe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building Generator\n",
    "\n",
    "def build_generator():\n",
    "    \n",
    "    noise_shape = (100,) # using which generator will generate images\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256,input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(1023))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape),activation='tanh')) ## np.prod multiplies 28,28 and 1 -> 784\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    noise = Input(shape=noise_shape)\n",
    "    output = model(noise)\n",
    "    \n",
    "    return Model(noise,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aad8ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### building Discriminator\n",
    "\n",
    "def build_discriminator():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)   # its a guess of a discriminator that image is real or fake\n",
    "    \n",
    "    return Model(img,validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d92fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining training function\n",
    "\n",
    "def train(epochs,batch_size=128,save_interval=100):\n",
    "    \n",
    "    (X_train,_),(_,_) = mnist.load_data()\n",
    "    \n",
    "    ## scaling images\n",
    "    X_train = X_train/255.\n",
    "    \n",
    "    X_train = np.expand_dims(X_train,axis=3)  # 28,28 -> 28,28,1\n",
    "    \n",
    "    half_batch = batch_size//2\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # ---------------------------------\n",
    "        ## First Training the Discriminator\n",
    "        # ---------------------------------\n",
    "        \n",
    "        # getting random images from X_train\n",
    "        idx = np.random.randint(0,X_train.shape[0],half_batch) # getting random half_bach indexes from 0 to 60k\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        \n",
    "        ## generating noise \n",
    "        noise = np.random.normal(0,1,(half_batch,100))  # It will generate half_batch,100 values between 0 and 1\n",
    "        \n",
    "        ## generating fake images\n",
    "        gen_imgs = generator(noise)\n",
    "        \n",
    "        ## Training the discrimainator on real and fake images separately\n",
    "        d_loss_real = discriminator.train_on_batch(imgs,np.ones((half_batch,1))) # paasing real images and telling discriminator that it is real by passing ones with it\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs,np.ones((half_batch,1))) # passing generated fake images and passing ones with it fool geenrator by saying it is real\n",
    "        \n",
    "        # averaging loss\n",
    "        d_loss = np.add(d_loss_real, d_loss_fake) * 0.5\n",
    "        \n",
    "        \n",
    "        #----------------------\n",
    "        ## Training Generator\n",
    "        #----------------------\n",
    "        \n",
    "        noise = np.random.normal(0,1,(batch_size,100))\n",
    "        \n",
    "        valid_y = np.array([1]*batch_size) # to fool the discriminator\n",
    "        \n",
    "        g_loss = combined.train_on_batch(noise,valid_y)\n",
    "        \n",
    "        print(f\"D loss {d_loss}, G loss {g_loss}\")\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "        \n",
    "\n",
    "        \n",
    "#This function saves our images for us to view\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0f454f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x00000187092CF4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "D loss [0.65544951 0.703125  ], G loss 0.543929398059845\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "D loss [0.47398478 1.        ], G loss 0.41101470589637756\n",
      "D loss [0.36653055 1.        ], G loss 0.28648287057876587\n",
      "D loss [0.28746971 1.        ], G loss 0.199311763048172\n",
      "D loss [0.22117603 1.        ], G loss 0.13194140791893005\n",
      "D loss [0.14443546 1.        ], G loss 0.0849492996931076\n",
      "D loss [0.09104678 1.        ], G loss 0.055805571377277374\n",
      "D loss [0.06528289 1.        ], G loss 0.03326776623725891\n",
      "D loss [0.03294604 1.        ], G loss 0.02242463082075119\n",
      "D loss [0.02711387 1.        ], G loss 0.015557728707790375\n",
      "D loss [0.01801869 1.        ], G loss 0.009629139676690102\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "D loss [0.01025968 1.        ], G loss 0.007471442688256502\n",
      "D loss [0.00842034 1.        ], G loss 0.005322286393493414\n",
      "D loss [0.00624797 1.        ], G loss 0.0044684880413115025\n",
      "D loss [0.005892 1.      ], G loss 0.00324054853990674\n",
      "D loss [0.00304573 1.        ], G loss 0.0024852966889739037\n",
      "D loss [0.00324333 1.        ], G loss 0.002259219530969858\n",
      "D loss [0.00177882 1.        ], G loss 0.0019006957300007343\n",
      "D loss [0.00260203 1.        ], G loss 0.001670817844569683\n",
      "D loss [0.00214841 1.        ], G loss 0.0013164043193683028\n",
      "D loss [0.00141128 1.        ], G loss 0.001177748665213585\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "D loss [0.00184451 1.        ], G loss 0.0009557814919389784\n",
      "D loss [0.0018394 1.       ], G loss 0.0009081860771402717\n",
      "D loss [0.00124795 1.        ], G loss 0.0008608349016867578\n",
      "D loss [0.00146973 1.        ], G loss 0.0007293082308024168\n",
      "D loss [0.00127303 1.        ], G loss 0.0007030427805148065\n",
      "D loss [9.76498355e-04 1.00000000e+00], G loss 0.0007692448562011123\n",
      "D loss [0.00102471 1.        ], G loss 0.0006202886579558253\n",
      "D loss [0.00110241 1.        ], G loss 0.0006442959420382977\n",
      "D loss [7.77717622e-04 1.00000000e+00], G loss 0.0005843354156240821\n",
      "D loss [0.00105373 1.        ], G loss 0.0005621020682156086\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "D loss [8.28265562e-04 1.00000000e+00], G loss 0.0005404391558840871\n",
      "D loss [0.00122989 1.        ], G loss 0.0005126182222738862\n",
      "D loss [5.93392528e-04 1.00000000e+00], G loss 0.0004593165358528495\n",
      "D loss [8.30196979e-04 1.00000000e+00], G loss 0.0004885026719421148\n",
      "D loss [8.2375198e-04 1.0000000e+00], G loss 0.0004501326475292444\n",
      "D loss [5.6004971e-04 1.0000000e+00], G loss 0.0004279819841030985\n",
      "D loss [5.92020966e-04 1.00000000e+00], G loss 0.00038627214962616563\n",
      "D loss [9.22746491e-04 1.00000000e+00], G loss 0.00040076070581562817\n",
      "D loss [8.44719514e-04 1.00000000e+00], G loss 0.0003965870419051498\n",
      "D loss [9.50287504e-04 1.00000000e+00], G loss 0.0003654878819361329\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "D loss [7.64781056e-04 1.00000000e+00], G loss 0.00038943259278312325\n",
      "D loss [6.34724245e-04 1.00000000e+00], G loss 0.00034428806975483894\n",
      "D loss [4.83801196e-04 1.00000000e+00], G loss 0.0003651520819403231\n",
      "D loss [5.35250583e-04 1.00000000e+00], G loss 0.0003887927741743624\n",
      "D loss [7.17917152e-04 1.00000000e+00], G loss 0.00035422947257757187\n",
      "D loss [6.35461183e-04 1.00000000e+00], G loss 0.0003515433636493981\n",
      "D loss [5.6214715e-04 1.0000000e+00], G loss 0.0002905799192376435\n",
      "D loss [5.375373e-04 1.000000e+00], G loss 0.0003113015554845333\n",
      "D loss [4.62620024e-04 1.00000000e+00], G loss 0.00027297838823869824\n",
      "D loss [7.10814464e-04 1.00000000e+00], G loss 0.0003155430022161454\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "D loss [5.23070252e-04 1.00000000e+00], G loss 0.00032499205553904176\n",
      "D loss [5.42480469e-04 1.00000000e+00], G loss 0.00034420896554365754\n",
      "D loss [7.16747818e-04 1.00000000e+00], G loss 0.00026510830502957106\n",
      "D loss [5.3241456e-04 1.0000000e+00], G loss 0.0002624398039188236\n",
      "D loss [6.0438423e-04 1.0000000e+00], G loss 0.0002932522911578417\n",
      "D loss [3.56863966e-04 1.00000000e+00], G loss 0.00030926818726584315\n",
      "D loss [3.78862795e-04 1.00000000e+00], G loss 0.0002617222198750824\n",
      "D loss [5.71932236e-04 1.00000000e+00], G loss 0.00030745004187338054\n",
      "D loss [4.0322385e-04 1.0000000e+00], G loss 0.00027036978281103075\n",
      "D loss [5.29593293e-04 1.00000000e+00], G loss 0.00024447025498375297\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "D loss [3.11140553e-04 1.00000000e+00], G loss 0.00023824407253414392\n",
      "D loss [5.36101245e-04 1.00000000e+00], G loss 0.0002822919050231576\n",
      "D loss [3.33318341e-04 1.00000000e+00], G loss 0.00023486747522838414\n",
      "D loss [4.60760173e-04 1.00000000e+00], G loss 0.0002409654698567465\n",
      "D loss [5.24518226e-04 1.00000000e+00], G loss 0.0002622675383463502\n",
      "D loss [5.13652718e-04 1.00000000e+00], G loss 0.00024832881172187626\n",
      "D loss [5.14769563e-04 1.00000000e+00], G loss 0.00021015033416915685\n",
      "D loss [4.03428741e-04 1.00000000e+00], G loss 0.0002456274232827127\n",
      "D loss [3.1165831e-04 1.0000000e+00], G loss 0.0002092077920679003\n",
      "D loss [3.69975343e-04 1.00000000e+00], G loss 0.00028798816492781043\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "D loss [4.7718588e-04 1.0000000e+00], G loss 0.000245397153776139\n",
      "D loss [5.96525017e-04 1.00000000e+00], G loss 0.00020059413509443402\n",
      "D loss [4.4366598e-04 1.0000000e+00], G loss 0.0002112576476065442\n",
      "D loss [2.17035347e-04 1.00000000e+00], G loss 0.0002025994035648182\n",
      "D loss [2.40897854e-04 1.00000000e+00], G loss 0.00021136521536391228\n",
      "D loss [3.70015856e-04 1.00000000e+00], G loss 0.00019854746642522514\n",
      "D loss [4.03552549e-04 1.00000000e+00], G loss 0.00019787038036156446\n",
      "D loss [3.41611507e-04 1.00000000e+00], G loss 0.00020984112052246928\n",
      "D loss [5.22703565e-04 1.00000000e+00], G loss 0.00019990447617601603\n",
      "D loss [3.05859052e-04 1.00000000e+00], G loss 0.0001865620433818549\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "D loss [4.70568804e-04 1.00000000e+00], G loss 0.00020479933300521225\n",
      "D loss [4.22305129e-04 1.00000000e+00], G loss 0.00020807913097087294\n",
      "D loss [3.97715325e-04 1.00000000e+00], G loss 0.00019226320728193969\n",
      "D loss [2.55644714e-04 1.00000000e+00], G loss 0.0001790771639207378\n",
      "D loss [3.8556115e-04 1.0000000e+00], G loss 0.00017869060684461147\n",
      "D loss [3.7709219e-04 1.0000000e+00], G loss 0.00017512479098513722\n",
      "D loss [4.96773209e-04 1.00000000e+00], G loss 0.000174068845808506\n",
      "D loss [2.19381414e-04 1.00000000e+00], G loss 0.0001744108449202031\n",
      "D loss [2.8504878e-04 1.0000000e+00], G loss 0.00017632413073442876\n",
      "D loss [4.82328891e-04 1.00000000e+00], G loss 0.00017671813839115202\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "D loss [2.16548775e-04 1.00000000e+00], G loss 0.0001849544933065772\n",
      "D loss [5.11462044e-04 1.00000000e+00], G loss 0.0001786161446943879\n",
      "D loss [2.59122011e-04 1.00000000e+00], G loss 0.0001730822114041075\n",
      "D loss [1.74289424e-04 1.00000000e+00], G loss 0.00017232733080163598\n",
      "D loss [1.54057328e-04 1.00000000e+00], G loss 0.000155648187501356\n",
      "D loss [2.54798753e-04 1.00000000e+00], G loss 0.00017826579278334975\n",
      "D loss [2.26061828e-04 1.00000000e+00], G loss 0.00016153660544659942\n",
      "D loss [5.9391385e-04 1.0000000e+00], G loss 0.0001521109079476446\n",
      "D loss [2.88562631e-04 1.00000000e+00], G loss 0.00016513679292984307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albiorix Technology\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## defining optimizer\n",
    "optimizer = Adam(0.0002)\n",
    "\n",
    "## discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy',])\n",
    "\n",
    "## Generator\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy',optimizer=optimizer) ## we are generating fake images, no need to track metrics\n",
    "\n",
    "\n",
    "# input to generator \n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# disabling discriminator training\n",
    "discriminator.trainable = False\n",
    "\n",
    "## validating generator output\n",
    "valid = discriminator(img)\n",
    "\n",
    "## stacking z and valid to give feedback to generate better images\n",
    "combined = Model(z,valid)\n",
    "combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "# training \n",
    "train(100,128,10)\n",
    "\n",
    "generator.save('gen_100_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06bd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991bc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc344a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
